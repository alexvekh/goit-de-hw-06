# Kafka Spark Streaming Alerts

Цей проєкт реалізує поточну обробку даних за допомогою Apache Spark Streaming та передає результати до Apache Kafka. Основна мета — обробка потокових даних, їх трансформація та надсилання до Kafka для подальшого аналізу або використання іншими сервісами.

## Технології

- Apache Spark — обробка потокових даних у режимі реального часу
- Apache Kafka — брокер повідомлень для передачі даних між системами
- Python — основна мова програмування

## Функціональність

- Зчитування даних з потоку Spark Streaming
- Обробка та трансформація отриманих даних
- Конвертація в JSON-формат
- Надсилання оброблених даних у Kafka

## Налаштування та запуск

### 1. Встановлення залежностей
Перед запуском переконайтесь, що у вас встановлені необхідні бібліотеки:

        pip install pyspark kafka-python

### 2. Запуск Kafka Broker

Перед запуском Spark Streaming необхідно запустити Kafka:

        zookeeper-server-start.sh config/zookeeper.properties &
        kafka-server-start.sh config/server.properties &

### 3. Запуск Spark Streaming

Запустіть скрипт, що обробляє потік даних:

        python kafka_alerts.py

## Файли

- configs.py - містить конфігураційні параметри для Kafka та Spark
- create_topics.py - скрипт для створення топіків у Kafka
- producer.py - генерація рандомних сенсорних данх та відправка повідомлень у Kafka
- alert_conditions.scv - файл із визначенням умов для генерації алертів
- kafka_alerts.py - основний скрипт обробки потокових даних у Spark та відправки їх у Kafka
- kafka_reader.py - споживач повідомлень Kafka, який читає та обробляє отримані дані
